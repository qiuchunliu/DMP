
报错： Error:scalac: bad option: '-make:transitive'
    解决：
    Scala自2.11版本开始，移除了弃用的-make选项，
    而idea的scala插件没有更新，
    因此，Scala由原先的2.10升级到2.11后，就出现了上述错误。
    关闭idea
    打开项目所在位置并cd .idea
    修改scala_compiler.xml文件
    删除掉参数行包含-make:transitive

文件的输出路径不能存在
    df.write.parquet(path)


sparkSql 在判断条件时，不识别 && ，需要使用 and 连接条件

case when 的使用
    "case devicetype when 1 then '手机' when 2 then '平板' else '其他' end as device,"
    注意 then 后边的 变量要加引号引起来，

mkString() 方法的使用
    val arr = Array[String]("3", "s", "r")
    println(arr.mkString(","))  // 3,s,r



基于RDD的分区，由于在spark中一个partition总是存储在一个excutor上，因此可以创建一个HBase连接，提交整个partition的内容
    rdd.foreachPartition { records =>
        val config = HBaseConfiguration.create
        config.set("hbase.zookeeper.property.clientPort", "2181")
        config.set("hbase.zookeeper.quorum", "a1,a2,a3")
        val connection = ConnectionFactory.createConnection(config)
        val table = connection.getTable(TableName.valueOf("rec:user_rec"))

        // 举个例子而已，真实的代码根据records来
        val list = new java.util.ArrayList[Put]
        for(i <- 0 until 10){
            val put = new Put(Bytes.toBytes(i.toString))
            put.addColumn(Bytes.toBytes("t"), Bytes.toBytes("aaaa"), Bytes.toBytes("1111"))
            list.add(put)
        }
        // 批量提交
        table.put(list)
        // 分区数据写入HBase后关闭连接
        table.close()
    }

